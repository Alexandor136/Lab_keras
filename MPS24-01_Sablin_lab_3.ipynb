{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f0d2b4c",
   "metadata": {},
   "source": [
    "Контрольные вопросы:\n",
    "\n",
    "1) В чем заключается идея внутреннего внимания?\n",
    "Идея внутреннего внимания (self-attention) в трансформерах заключается в том, чтобы позволить модели сосредотачиваться на различных частях входной последовательности, когда она обрабатывает каждое слово (или токен). Это важно для понимания контекста и взаимосвязей между словами в предложении. \n",
    "\n",
    "2) Что такое Multi-head attention?\n",
    "Multi-head attention (многоголовое внимание) — это расширение механизма внутреннего внимания, которое позволяет модели одновременно использовать несколько \"голов\" внимания для обработки информации. Это улучшает способность модели усваивать различные аспекты контекста и позволяет более эффективно извлекать семантические зависимости.\n",
    "\n",
    "Вместо того чтобы использовать одно представление (вектор) для вычисления внимания, входные векторы Query, Key, и Value разделяются на несколько \"голов\". Каждая голова имеет свои собственные параметры и обучается независимо, что позволяет изучать разные подпространства.\n",
    "\n",
    "3) Можно ли использовать отдельно кодировщик и декодировщик трансформера? Для каких целей?\n",
    "Да, можно испольщовать кодировщик и декадировщик отдельно. \n",
    "    Кодировщик используется для задач, где необходимо извлечение информации и понимание входных данных:\n",
    "    1) Классификация текстов\n",
    "    2) Аннотация сущностей\n",
    "    3) Извлечение информации\n",
    "    4) Понимание контекста\n",
    "\n",
    "    Декодировщик используется для задач, где необходима  генерация последовательности, например:\n",
    "    1) Генерация текста\n",
    "    2) Создание описаний\n",
    "    3) Диалоговые системы\n",
    "\n",
    "4) Как представляются исходные данные для модели трансформера?\n",
    "Представление исходных данных для модели трансформера осуществляется через несколько ключевых шагов:\n",
    "    1) Токенизация  \n",
    "    2) Преобразование токенов в числовые идентификаторы\n",
    "    3) Добавление специальных токенов (необязательно)\n",
    "    4) Паддинг последовательностей\n",
    "    5) Кодирование позиций\n",
    "    6) Векторизация\n",
    "\n",
    "5) К какому классу задач относится задача генерации текста? Какие основные принципы её решения вам известны?\n",
    "Задача генерации текста относится к классу задач обработки естественного языка (NLP) и обычно рассматривается как подзадача генерации языка или языкового моделирования. Основная цель генерации текста — создание последовательностей слов, которые имеют смысл и соответствуют определенному контексту или тематике.\n",
    "   1) Использование языковых моделей (Language Models, LM)\n",
    "   2) Архитектуры на основе трансформеров\n",
    "   3) Обучение на больших данных\n",
    "   4) Стратегии генерации\n",
    "   5) Контроль качества и согласованности"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa075d25",
   "metadata": {},
   "source": [
    "Практические задания:\n",
    "1) Решите задачу классификации текста на русском языке (или с применением датасета отличного от предложенного в работе) при помощи модели кодировщика Трансформера.\n",
    "2) Выполните предобработку набора данных для подачи на вход нейронной сети в задаче машинного перевода.\n",
    "3) Примените описанные здесь модели RNN и Transformer для задачи перевода (например английский - русский) или генерации текста (ответы на вопросы).\n",
    "4) Сравните результаты моделей RNN и Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d60c19",
   "metadata": {},
   "source": [
    "1) Решите задачу классификации текста на русском языке (или с применением датасета отличного от предложенного в работе) при помощи модели кодировщика Трансформера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c248c0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a047a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tripadvisor_hotel_reviews.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "412627cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20491, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a420d63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Review    0\n",
       "Rating    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e019bb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nice hotel expensive parking got good deal sta...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok nothing special charge diamond member hilto...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nice rooms not 4* experience hotel monaco seat...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unique, great stay, wonderful time hotel monac...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great stay great stay, went seahawk game aweso...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>love monaco staff husband stayed hotel crazy w...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cozy stay rainy city, husband spent 7 nights m...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>excellent staff, housekeeping quality hotel ch...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hotel stayed hotel monaco cruise, rooms genero...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>excellent stayed hotel monaco past w/e delight...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  nice hotel expensive parking got good deal sta...       4\n",
       "1  ok nothing special charge diamond member hilto...       2\n",
       "2  nice rooms not 4* experience hotel monaco seat...       3\n",
       "3  unique, great stay, wonderful time hotel monac...       5\n",
       "4  great stay great stay, went seahawk game aweso...       5\n",
       "5  love monaco staff husband stayed hotel crazy w...       5\n",
       "6  cozy stay rainy city, husband spent 7 nights m...       5\n",
       "7  excellent staff, housekeeping quality hotel ch...       4\n",
       "8  hotel stayed hotel monaco cruise, rooms genero...       5\n",
       "9  excellent stayed hotel monaco past w/e delight...       5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b60f0602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(rating):\n",
    "    if rating < 3:\n",
    "        return 0\n",
    "    elif rating == 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "df['Label'] = df['Rating'].apply(label)\n",
    "df['Label'] = df['Label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba5acbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nice hotel expensive parking got good deal sta...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok nothing special charge diamond member hilto...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nice rooms not 4* experience hotel monaco seat...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unique, great stay, wonderful time hotel monac...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great stay great stay, went seahawk game aweso...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>love monaco staff husband stayed hotel crazy w...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cozy stay rainy city, husband spent 7 nights m...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>excellent staff, housekeeping quality hotel ch...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hotel stayed hotel monaco cruise, rooms genero...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>excellent stayed hotel monaco past w/e delight...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating  Label\n",
       "0  nice hotel expensive parking got good deal sta...       4      2\n",
       "1  ok nothing special charge diamond member hilto...       2      0\n",
       "2  nice rooms not 4* experience hotel monaco seat...       3      1\n",
       "3  unique, great stay, wonderful time hotel monac...       5      2\n",
       "4  great stay great stay, went seahawk game aweso...       5      2\n",
       "5  love monaco staff husband stayed hotel crazy w...       5      2\n",
       "6  cozy stay rainy city, husband spent 7 nights m...       5      2\n",
       "7  excellent staff, housekeeping quality hotel ch...       4      2\n",
       "8  hotel stayed hotel monaco cruise, rooms genero...       5      2\n",
       "9  excellent stayed hotel monaco past w/e delight...       5      2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74f1cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['Review']\n",
    "y = df['Label']\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.75, stratify=y, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4bf7e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "test_df = pd.concat([X_valid, y_valid], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6177422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train.csv', index=False)\n",
    "test_df.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eaf39bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aa-sablin/env_lab/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = 'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c753747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 15368 examples [00:00, 132946.26 examples/s]\n",
      "Generating test split: 5123 examples [00:00, 137670.15 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\n",
    "    'train': r'train.csv',\n",
    "    'test': r'test.csv'\n",
    "}\n",
    "dataset = load_dataset('csv', data_files=data_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d1092fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Review', 'Label'],\n",
       "        num_rows: 15368\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Review', 'Label'],\n",
       "        num_rows: 5123\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07bb484f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/15368 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 15368/15368 [00:02<00:00, 7398.52 examples/s]\n",
      "Map: 100%|██████████| 5123/5123 [00:00<00:00, 8702.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example['Review'], truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e18a5472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Review', 'Label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 15368\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Review', 'Label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 5123\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2cbfe8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 22:27:19.162216: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-30 22:27:19.166775: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-30 22:27:19.173104: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748618839.185133   16260 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748618839.188566   16260 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748618839.197438   16260 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748618839.197450   16260 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748618839.197452   16260 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748618839.197453   16260 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-30 22:27:19.200515: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "276c40b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c44644b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aa-sablin/env_lab/lib/python3.12/site-packages/datasets/arrow_dataset.py:400: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n",
      "E0000 00:00:1748618840.701374   16260 cuda_executor.cc:1228] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1748618840.711840   16260 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "tf_train_dataset = tokenized_dataset['train'].to_tf_dataset(\n",
    "    columns=['input_ids', 'attention_mask'],\n",
    "    label_cols=['Label'],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=10\n",
    ")\n",
    "\n",
    "tf_test_dataset = tokenized_dataset['test'].to_tf_dataset(\n",
    "    columns=['input_ids', 'attention_mask'],\n",
    "    label_cols=['Label'],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a091d21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, BatchNormalization, Dropout, GaussianDropout, Bidirectional, Embedding, LSTM\n",
    "from tensorflow.keras.optimizers import Adam, SGD, Nadam\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "def build_model(num_classes=3):\n",
    "    inputs = Input(shape=(None,), dtype='int32', name='input_ids')\n",
    "    x = Embedding(input_dim=vocab_size, output_dim=50)(inputs)\n",
    "    x = Bidirectional(LSTM(64))(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    outputs = Dense(3, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8961b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "def make_callbacks():\n",
    "    lr_callback = ReduceLROnPlateau(\n",
    "            monitor='val_loss',     \n",
    "            factor=0.5,              \n",
    "            patience=5,              \n",
    "            verbose=1,               \n",
    "            min_lr=1e-6            \n",
    "        )\n",
    "    \n",
    "    early_stop = EarlyStopping(\n",
    "        patience=15, \n",
    "        monitor='val_loss', \n",
    "        restore_best_weights=True, \n",
    "        mode='min'\n",
    "    )\n",
    "\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=\"best_model.keras\",  \n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    return [lr_callback, early_stop, checkpoint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb590959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1537/1537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 104ms/step - accuracy: 0.7386 - loss: 0.7442 - val_accuracy: 0.8378 - val_loss: 0.4359 - learning_rate: 1.0000e-04\n",
      "Epoch 2/5\n",
      "\u001b[1m1537/1537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 106ms/step - accuracy: 0.8561 - loss: 0.3807 - val_accuracy: 0.8528 - val_loss: 0.3929 - learning_rate: 1.0000e-04\n",
      "Epoch 3/5\n",
      "\u001b[1m1537/1537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 108ms/step - accuracy: 0.8809 - loss: 0.3003 - val_accuracy: 0.8513 - val_loss: 0.3944 - learning_rate: 1.0000e-04\n",
      "Epoch 4/5\n",
      "\u001b[1m1537/1537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 107ms/step - accuracy: 0.9050 - loss: 0.2447 - val_accuracy: 0.8497 - val_loss: 0.4155 - learning_rate: 1.0000e-04\n",
      "Epoch 5/5\n",
      "\u001b[1m1537/1537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 108ms/step - accuracy: 0.9251 - loss: 0.2011 - val_accuracy: 0.8356 - val_loss: 0.4519 - learning_rate: 1.0000e-04\n",
      "\u001b[1m513/513\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - accuracy: 0.8517 - loss: 0.3951\n",
      "Test Acc.: 85.32%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "dnn = build_model()\n",
    "\n",
    "optimizer = Nadam(learning_rate=0.0001)\n",
    "    \n",
    "dnn.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=SparseCategoricalCrossentropy(),\n",
    "        metrics=['accuracy']    \n",
    "    )\n",
    "\n",
    "dnn.fit(\n",
    "    tf_train_dataset, \n",
    "    validation_data=tf_test_dataset, \n",
    "    batch_size=100,\n",
    "    epochs=5,\n",
    "    callbacks=make_callbacks(),   \n",
    ")\n",
    "\n",
    "test_results = dnn.evaluate(tf_test_dataset)\n",
    "print(\"Test Acc.: {:.2f}%\".format(test_results[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe1ba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Review: perfect money looking safe comfortable reasonably priced hotel, hotel room clean free parking plus easy park, staff helpful nice deal, good location away crowd wharf close, restaurants close walgreens shopping fridge heat microwave,  \n",
      "Predicted Label: [2]\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Review: passable nothing special hotel generally clean really needs updating, bathroom strange not typically, shower consists curtain drain floor corner room, terrace beautiful combined bottle wine great way spend afternoon, lack included breakfast left feeling room overpriced compared places stayed italy\n",
      "Predicted Label: [0]\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Review: worth money rest, hotel not, going big easy relax pampered need stay, going party city, went halloween celebration insane, hotel block bourbon st. friends stay bourbon twice not away noise music clock, nice quiet nap mid day nice prepare nightlife, rooms need updating hotel fairly clean staff nice helpful times, rooms ready early issue toilet kept running fixed 10 minutes, room service tgi friday hold home eat gumbo jumbalya, pool small clean seeing no kids, stay liver recovers\n",
      "Predicted Label: [2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_texts = [\n",
    "    \"perfect money looking safe comfortable reasonably priced hotel, hotel room clean free parking plus easy park, staff helpful nice deal, good location away crowd wharf close, restaurants close walgreens shopping fridge heat microwave,  \",  # 2 review\n",
    "    \"passable nothing special hotel generally clean really needs updating, bathroom strange not typically, shower consists curtain drain floor corner room, terrace beautiful combined bottle wine great way spend afternoon, lack included breakfast left feeling room overpriced compared places stayed italy\", # 0\n",
    "    \"worth money rest, hotel not, going big easy relax pampered need stay, going party city, went halloween celebration insane, hotel block bourbon st. friends stay bourbon twice not away noise music clock, nice quiet nap mid day nice prepare nightlife, rooms need updating hotel fairly clean staff nice helpful times, rooms ready early issue toilet kept running fixed 10 minutes, room service tgi friday hold home eat gumbo jumbalya, pool small clean seeing no kids, stay liver recovers\" # 2\n",
    "]\n",
    "\n",
    "for review in new_texts:\n",
    "    new_encoding = tokenizer(review, truncation=True, padding=True, return_tensors='tf')\n",
    "\n",
    "    new_prediction = dnn.predict(new_encoding['input_ids'])\n",
    "\n",
    "    predicted_label = np.argmax(new_prediction, axis=1)\n",
    "\n",
    "    print(f\"Review: {review}\\nPredicted Label: {predicted_label}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc2e105",
   "metadata": {},
   "source": [
    "2) Выполните предобработку набора данных для подачи на вход нейронной сети в задаче машинного перевода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471fb792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Загрузка и подготовка данных из CSV\n",
    "csv_file = \"data_translate.csv\"  \n",
    "df = pd.read_csv(csv_file, sep=\"\\t\")  \n",
    "\n",
    "# Формируем пары (англ, рус) с добавлением токенов начала и конца для русского\n",
    "pairs = []\n",
    "for _, row in df.iterrows():\n",
    "    english = row[\"en\"].strip()\n",
    "    russian = \"[start] \" + row[\"ru\"].strip() + \" [end]\"\n",
    "    if english and russian:  # Проверяем, что строки не пустые\n",
    "        pairs.append((english, russian))\n",
    "\n",
    "# Перемешивание данных\n",
    "random.shuffle(pairs)\n",
    "\n",
    "# Разделение на обучающую, валидационную и тестовую выборки\n",
    "num_val_samples = int(0.15 * len(pairs))\n",
    "num_train_samples = len(pairs) - 2 * num_val_samples\n",
    "train_pairs = pairs[:num_train_samples]\n",
    "val_pairs = pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = pairs[num_train_samples + num_val_samples:]\n",
    "\n",
    "# Векторизация текста\n",
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "\n",
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "\n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "target_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "\n",
    "# Обучение векторизаторов на тренировочных данных\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\n",
    "train_russian_texts = [pair[1] for pair in train_pairs]\n",
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_russian_texts)\n",
    "\n",
    "# Создание датасета\n",
    "batch_size = 64\n",
    "\n",
    "def format_dataset(eng, rus):\n",
    "    eng = source_vectorization(eng)\n",
    "    rus = target_vectorization(rus)\n",
    "    return ({\n",
    "        \"english\": eng,\n",
    "        \"russian\": rus[:, :-1],\n",
    "    }, rus[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    # Проверяем, что пары не пустые\n",
    "    if not pairs:\n",
    "        raise ValueError(\"Pairs are empty.\")\n",
    "    \n",
    "    eng_texts, rus_texts = zip(*pairs)\n",
    "    \n",
    "    # Убедимся, что данные в правильном формате\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(eng_texts), list(rus_texts)))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset.shuffle(2048).prefetch(tf.data.AUTOTUNE).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)\n",
    "\n",
    "# Сохранение словарей векторизаторов для обучения\n",
    "source_vocab = source_vectorization.get_vocabulary()\n",
    "target_vocab = target_vectorization.get_vocabulary()\n",
    "\n",
    "# Сохранение в текстовые файлы\n",
    "with open(\"source_vocab.txt\", \"w\") as f:\n",
    "    for word in source_vocab:\n",
    "        f.write(f\"{word}\\n\")\n",
    "\n",
    "with open(\"target_vocab.txt\", \"w\") as f:\n",
    "    for word in target_vocab:\n",
    "        f.write(f\"{word}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa0074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Загрузка и подготовка данных из CSV\n",
    "csv_file = \"data_translate.csv\"  \n",
    "df = pd.read_csv(csv_file, sep=\"\\t\")  \n",
    "\n",
    "# Формируем пары (англ, рус) с добавлением токенов начала и конца для русского\n",
    "pairs = []\n",
    "for _, row in df.iterrows():\n",
    "    english = row[\"en\"].strip()\n",
    "    russian = \"[start] \" + row[\"ru\"].strip() + \" [end]\"\n",
    "    if english and russian:  # Проверяем, что строки не пустые\n",
    "        pairs.append((english, russian))\n",
    "\n",
    "# Перемешивание данных\n",
    "random.shuffle(pairs)\n",
    "\n",
    "# Разделение на обучающую, валидационную и тестовую выборки\n",
    "num_val_samples = int(0.15 * len(pairs))\n",
    "num_train_samples = len(pairs) - 2 * num_val_samples\n",
    "train_pairs = pairs[:num_train_samples]\n",
    "val_pairs = pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = pairs[num_train_samples + num_val_samples:]\n",
    "\n",
    "# Векторизация текста\n",
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "\n",
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "\n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "target_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "\n",
    "# Обучение векторизаторов на тренировочных данных\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\n",
    "train_russian_texts = [pair[1] for pair in train_pairs]\n",
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_russian_texts)\n",
    "\n",
    "# Создание датасета\n",
    "batch_size = 64\n",
    "\n",
    "def format_dataset(eng, rus):\n",
    "    eng = source_vectorization(eng)\n",
    "    rus = target_vectorization(rus)\n",
    "    return ({\n",
    "        \"english\": eng,\n",
    "        \"russian\": rus[:, :-1],\n",
    "    }, rus[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    # Проверяем, что пары не пустые\n",
    "    if not pairs:\n",
    "        raise ValueError(\"Pairs are empty.\")\n",
    "    \n",
    "    eng_texts, rus_texts = zip(*pairs)\n",
    "    \n",
    "    # Убедимся, что данные в правильном формате\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(eng_texts), list(rus_texts)))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset.shuffle(2048).prefetch(tf.data.AUTOTUNE).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)\n",
    "\n",
    "# Сохранение словарей векторизаторов для обучения\n",
    "source_vocab = source_vectorization.get_vocabulary()\n",
    "target_vocab = target_vectorization.get_vocabulary()\n",
    "\n",
    "# Сохранение в текстовые файлы\n",
    "with open(\"source_vocab.txt\", \"w\") as f:\n",
    "    for word in source_vocab:\n",
    "        f.write(f\"{word}\\n\")\n",
    "\n",
    "with open(\"target_vocab.txt\", \"w\") as f:\n",
    "    for word in target_vocab:\n",
    "        f.write(f\"{word}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5cb33b",
   "metadata": {},
   "source": [
    "Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d8b28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aa-sablin/env_lab/lib/python3.12/site-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'transformer_encoder_10' (of type TransformerEncoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 141ms/step - accuracy: 0.2522 - loss: 5.2702 - val_accuracy: 0.1167 - val_loss: 3.8271\n",
      "Epoch 2/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - accuracy: 0.1238 - loss: 3.7388 - val_accuracy: 0.1233 - val_loss: 3.4679\n",
      "Epoch 3/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - accuracy: 0.1259 - loss: 3.3839 - val_accuracy: 0.1323 - val_loss: 3.2106\n",
      "Epoch 4/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - accuracy: 0.1367 - loss: 2.9837 - val_accuracy: 0.1482 - val_loss: 2.7309\n",
      "Epoch 5/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - accuracy: 0.1519 - loss: 2.4952 - val_accuracy: 0.1545 - val_loss: 2.5144\n",
      "Epoch 6/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 52ms/step - accuracy: 0.1584 - loss: 2.1911 - val_accuracy: 0.1571 - val_loss: 2.3943\n",
      "Epoch 7/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 53ms/step - accuracy: 0.1642 - loss: 1.9488 - val_accuracy: 0.1587 - val_loss: 2.2854\n",
      "Epoch 8/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 53ms/step - accuracy: 0.1686 - loss: 1.7573 - val_accuracy: 0.1613 - val_loss: 2.1935\n",
      "Epoch 9/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 53ms/step - accuracy: 0.1746 - loss: 1.5857 - val_accuracy: 0.1621 - val_loss: 2.1443\n",
      "Epoch 10/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 53ms/step - accuracy: 0.1772 - loss: 1.4477 - val_accuracy: 0.1625 - val_loss: 2.0863\n",
      "Epoch 11/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 54ms/step - accuracy: 0.1797 - loss: 1.3285 - val_accuracy: 0.1641 - val_loss: 2.0626\n",
      "Epoch 12/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 53ms/step - accuracy: 0.1821 - loss: 1.2166 - val_accuracy: 0.1639 - val_loss: 2.0845\n",
      "Epoch 13/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 53ms/step - accuracy: 0.1848 - loss: 1.1352 - val_accuracy: 0.1650 - val_loss: 2.0447\n",
      "Epoch 14/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 53ms/step - accuracy: 0.1868 - loss: 1.0636 - val_accuracy: 0.1648 - val_loss: 2.0365\n",
      "Epoch 15/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 54ms/step - accuracy: 0.1891 - loss: 1.0010 - val_accuracy: 0.1655 - val_loss: 2.0408\n",
      "Epoch 16/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 53ms/step - accuracy: 0.1905 - loss: 0.9472 - val_accuracy: 0.1642 - val_loss: 2.0551\n",
      "Epoch 17/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 53ms/step - accuracy: 0.1919 - loss: 0.9133 - val_accuracy: 0.1652 - val_loss: 2.0679\n",
      "Epoch 18/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 53ms/step - accuracy: 0.1932 - loss: 0.8770 - val_accuracy: 0.1639 - val_loss: 2.0668\n",
      "Epoch 19/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 53ms/step - accuracy: 0.1947 - loss: 0.8484 - val_accuracy: 0.1656 - val_loss: 2.0557\n",
      "Epoch 20/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 53ms/step - accuracy: 0.1962 - loss: 0.8195 - val_accuracy: 0.1654 - val_loss: 2.0913\n",
      "Epoch 21/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 53ms/step - accuracy: 0.1969 - loss: 0.7920 - val_accuracy: 0.1664 - val_loss: 2.0737\n",
      "Epoch 22/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 54ms/step - accuracy: 0.1980 - loss: 0.7743 - val_accuracy: 0.1658 - val_loss: 2.0775\n",
      "Epoch 23/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 53ms/step - accuracy: 0.1987 - loss: 0.7519 - val_accuracy: 0.1661 - val_loss: 2.1093\n",
      "Epoch 24/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 53ms/step - accuracy: 0.1994 - loss: 0.7446 - val_accuracy: 0.1650 - val_loss: 2.1275\n",
      "Epoch 25/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 53ms/step - accuracy: 0.2002 - loss: 0.7388 - val_accuracy: 0.1641 - val_loss: 2.1552\n",
      "Epoch 26/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 53ms/step - accuracy: 0.2004 - loss: 0.7361 - val_accuracy: 0.1655 - val_loss: 2.1225\n",
      "Epoch 27/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 54ms/step - accuracy: 0.2006 - loss: 0.7286 - val_accuracy: 0.1653 - val_loss: 2.1637\n",
      "Epoch 28/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 54ms/step - accuracy: 0.2007 - loss: 0.7291 - val_accuracy: 0.1657 - val_loss: 2.1658\n",
      "Epoch 29/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 54ms/step - accuracy: 0.2015 - loss: 0.7251 - val_accuracy: 0.1656 - val_loss: 2.1737\n",
      "Epoch 30/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 54ms/step - accuracy: 0.2018 - loss: 0.7228 - val_accuracy: 0.1658 - val_loss: 2.1740\n",
      "Модель сохранена в формате Keras.\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.1663 - loss: 2.1862\n",
      "Тестовая потеря: 2.2143, Тестовая точность: 0.1653\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential([\n",
    "            layers.Dense(dense_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential([\n",
    "            layers.Dense(dense_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "        else:\n",
    "            padding_mask = mask\n",
    "        attention_output_1 = self.attention_1(query=inputs, value=inputs, key=inputs, attention_mask=causal_mask)\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "        attention_output_2 = self.attention_2(query=attention_output_1, value=encoder_outputs, key=encoder_outputs, attention_mask=padding_mask)\n",
    "        attention_output_2 = self.layernorm_2(attention_output_1 + attention_output_2)\n",
    "        proj_output = self.dense_proj(attention_output_2)\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)\n",
    "\n",
    "class EmbeddedLayer(keras.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.positional_embedding = PositionalEmbedding(sequence_length, input_dim, output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.positional_embedding(inputs)\n",
    "\n",
    "# Параметры модели\n",
    "embed_dim = 256\n",
    "dense_dim = 2048\n",
    "num_heads = 8\n",
    "sequence_length = 20  # Пример длины последовательности\n",
    "vocab_size = 15000  # Размер словаря, соответствующий векторизации\n",
    "\n",
    "# Входные данные\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "embedded = EmbeddedLayer(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(embedded)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"russian\")\n",
    "embedded_decoder = EmbeddedLayer(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(embedded_decoder, encoder_outputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "\n",
    "# Создание и компиляция модели\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "transformer.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Обучение модели\n",
    "transformer.fit(train_ds, epochs=30, validation_data=val_ds)\n",
    "\n",
    "# Сохранение модели в формате Keras\n",
    "transformer.save(\"transformer_model.keras\")\n",
    "print(\"Модель сохранена в формате Keras.\")\n",
    "\n",
    "# Проверка на тестовых данных\n",
    "test_ds = make_dataset(test_pairs)\n",
    "\n",
    "# Оценка модели на тестовых данных\n",
    "test_loss, test_accuracy = transformer.evaluate(test_ds)\n",
    "print(f\"Тестовая потеря: {test_loss:.4f}, Тестовая точность: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f493be77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Input: I need it now.\n",
      "Output: [start] Мне нужно сейчас [end]\n",
      "------------------------------\n",
      "Input: Check this out.\n",
      "Output: [start] Проверьте это [end]\n",
      "------------------------------\n",
      "Input: Get in line.\n",
      "Output: [start] Будьте на строчку [end]\n",
      "------------------------------\n",
      "Input: Sit down.\n",
      "Output: [start] Сядь [end]\n",
      "------------------------------\n",
      "Input: They hired me.\n",
      "Output: [start] Они взяли на работу [end]\n",
      "------------------------------\n",
      "Input: Be realistic.\n",
      "Output: [start] Будь реалистом [end]\n",
      "------------------------------\n",
      "Input: Get a job.\n",
      "Output: [start] Найди работу [end]\n",
      "------------------------------\n",
      "Input: It's outdated.\n",
      "Output: [start] Это устарело [end]\n",
      "------------------------------\n",
      "Input: Put it down.\n",
      "Output: [start] Поставьте её [end]\n",
      "------------------------------\n",
      "Input: I admire you.\n",
      "Output: [start] Я восхищаюсь Томом [end]\n",
      "------------------------------\n",
      "Input: Let's eat out.\n",
      "Output: [start] Давай пойдём куданибудь поедим [end]\n",
      "------------------------------\n",
      "Input: Who took it?\n",
      "Output: [start] Кто её взял [end]\n",
      "------------------------------\n",
      "Input: Tom fired me.\n",
      "Output: [start] Том меня уволен [end]\n",
      "------------------------------\n",
      "Input: I'll leave.\n",
      "Output: [start] Я уйду [end]\n",
      "------------------------------\n",
      "Input: When's dinner?\n",
      "Output: [start] Когда ужин [end]\n",
      "------------------------------\n",
      "Input: We all die.\n",
      "Output: [start] Мы все умереть [end]\n",
      "------------------------------\n",
      "Input: Please fix it.\n",
      "Output: [start] Почините её пожалуйста [end]\n",
      "------------------------------\n",
      "Input: I want to quit.\n",
      "Output: [start] Я хочу уволился [end]\n",
      "------------------------------\n",
      "Input: I'm sleepy!\n",
      "Output: [start] Я сонный [end]\n",
      "------------------------------\n",
      "Input: Is that true?\n",
      "Output: [start] Это правда [end]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "\n",
    "max_decoded_sentence_length = 20  # максимальная длина для вывода\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    # Векторизуем входное предложение (английский текст)\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    # Начинаем выходную последовательность со стартового токена (пример: \"[start]\")\n",
    "    decoded_sentence = \"[start]\"\n",
    "    \n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        # Токенизируем уже сгенерированное предложение (декодер принимает все токены, кроме последнего)\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n",
    "        \n",
    "        # Модель предсказывает следующий токен\n",
    "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "        \n",
    "        # Берём вероятности для следующего токена на позиции i\n",
    "        predicted_id = np.argmax(predictions[0, i, :])\n",
    "        \n",
    "        # Получаем слово по индексу\n",
    "        predicted_word = spa_index_lookup.get(predicted_id, \"[unk]\")\n",
    "        \n",
    "        # Добавляем слово к выходной последовательности\n",
    "        decoded_sentence += \" \" + predicted_word\n",
    "        \n",
    "        # Останавливаемся, если встретили токен \"[end]\"\n",
    "        if predicted_word == \"[end]\":\n",
    "            break\n",
    "    \n",
    "    return decoded_sentence\n",
    "\n",
    "# Демонстрация вывода 20 различных примеров из тестового набора\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Input:\", input_sentence)\n",
    "    print(\"Output:\", decode_sequence(input_sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a305797",
   "metadata": {},
   "source": [
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592ea0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 40ms/step - accuracy: 0.3817 - loss: 5.4594 - val_accuracy: 0.1209 - val_loss: 3.5592\n",
      "Epoch 2/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - accuracy: 0.1218 - loss: 3.3540 - val_accuracy: 0.1277 - val_loss: 3.2896\n",
      "Epoch 3/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.1293 - loss: 2.9866 - val_accuracy: 0.1343 - val_loss: 3.1037\n",
      "Epoch 4/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.1350 - loss: 2.6644 - val_accuracy: 0.1367 - val_loss: 3.0080\n",
      "Epoch 5/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - accuracy: 0.1392 - loss: 2.3787 - val_accuracy: 0.1396 - val_loss: 2.9084\n",
      "Epoch 6/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step - accuracy: 0.1435 - loss: 2.1107 - val_accuracy: 0.1424 - val_loss: 2.8352\n",
      "Epoch 7/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - accuracy: 0.1480 - loss: 1.8601 - val_accuracy: 0.1441 - val_loss: 2.7825\n",
      "Epoch 8/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.1531 - loss: 1.6344 - val_accuracy: 0.1462 - val_loss: 2.7430\n",
      "Epoch 9/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.1582 - loss: 1.4341 - val_accuracy: 0.1472 - val_loss: 2.7403\n",
      "Epoch 10/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.1632 - loss: 1.2638 - val_accuracy: 0.1489 - val_loss: 2.7003\n",
      "Epoch 11/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.1694 - loss: 1.0963 - val_accuracy: 0.1498 - val_loss: 2.6880\n",
      "Epoch 12/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.1754 - loss: 0.9520 - val_accuracy: 0.1500 - val_loss: 2.6926\n",
      "Epoch 13/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.1812 - loss: 0.8229 - val_accuracy: 0.1509 - val_loss: 2.6898\n",
      "Epoch 14/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step - accuracy: 0.1872 - loss: 0.7129 - val_accuracy: 0.1515 - val_loss: 2.6959\n",
      "Epoch 15/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.1920 - loss: 0.6252 - val_accuracy: 0.1519 - val_loss: 2.6856\n",
      "Epoch 16/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.1955 - loss: 0.5605 - val_accuracy: 0.1529 - val_loss: 2.7010\n",
      "Epoch 17/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.1979 - loss: 0.5077 - val_accuracy: 0.1538 - val_loss: 2.7151\n",
      "Epoch 18/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step - accuracy: 0.2008 - loss: 0.4562 - val_accuracy: 0.1535 - val_loss: 2.7291\n",
      "Epoch 19/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step - accuracy: 0.2032 - loss: 0.4080 - val_accuracy: 0.1543 - val_loss: 2.7131\n",
      "Epoch 20/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step - accuracy: 0.2047 - loss: 0.3790 - val_accuracy: 0.1540 - val_loss: 2.7370\n",
      "Epoch 21/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.2066 - loss: 0.3449 - val_accuracy: 0.1532 - val_loss: 2.7439\n",
      "Epoch 22/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.2078 - loss: 0.3222 - val_accuracy: 0.1533 - val_loss: 2.7672\n",
      "Epoch 23/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.2082 - loss: 0.3089 - val_accuracy: 0.1531 - val_loss: 2.7909\n",
      "Epoch 24/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - accuracy: 0.2077 - loss: 0.3073 - val_accuracy: 0.1538 - val_loss: 2.8111\n",
      "Epoch 25/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - accuracy: 0.2080 - loss: 0.2986 - val_accuracy: 0.1543 - val_loss: 2.8058\n",
      "Epoch 26/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - accuracy: 0.2076 - loss: 0.2920 - val_accuracy: 0.1541 - val_loss: 2.7876\n",
      "Epoch 27/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - accuracy: 0.2080 - loss: 0.2796 - val_accuracy: 0.1538 - val_loss: 2.7820\n",
      "Epoch 28/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - accuracy: 0.2076 - loss: 0.2795 - val_accuracy: 0.1542 - val_loss: 2.7926\n",
      "Epoch 29/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - accuracy: 0.2073 - loss: 0.2791 - val_accuracy: 0.1544 - val_loss: 2.7946\n",
      "Epoch 30/30\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - accuracy: 0.2066 - loss: 0.2811 - val_accuracy: 0.1545 - val_loss: 2.8000\n",
      "Модель сохранена в формате Keras.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "\n",
    "# Создаем модель \n",
    "input_english = layers.Input(shape=(sequence_length,), name=\"english\")\n",
    "input_russian = layers.Input(shape=(sequence_length,), name=\"russian\")\n",
    "\n",
    "# Встраивание английских слов\n",
    "x = layers.Embedding(input_dim=len(source_vocab), output_dim=embedding_dim, mask_zero=True)(input_english)\n",
    "\n",
    "# Кодировщик\n",
    "encoder_output = layers.LSTM(units)(x)\n",
    "\n",
    "# Встраивание русских слов (вход декодера)\n",
    "decoder_embedding = layers.Embedding(input_dim=len(target_vocab), output_dim=embedding_dim, mask_zero=True)(input_russian)\n",
    "\n",
    "# Декодер с инициализацией состояния из кодировщика\n",
    "decoder_lstm = layers.LSTM(units, return_sequences=True)\n",
    "decoder_output = decoder_lstm(decoder_embedding, initial_state=[encoder_output, encoder_output])\n",
    "\n",
    "# Выходной слой\n",
    "output = layers.Dense(len(target_vocab), activation='softmax')(decoder_output)\n",
    "\n",
    "model = models.Model(inputs=[input_english, input_russian], outputs=output)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Обучение модели \n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=30,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Сохранение модели в формате Keras\n",
    "transformer.save(\"RNN_model.keras\")\n",
    "print(\"Модель сохранена в формате Keras.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fca2fce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  I'm wasted.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Output: Я измождена\n",
      "----------------------------------------\n",
      "Input:  Is Tom coming?\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Output: Том идёт\n",
      "----------------------------------------\n",
      "Input:  Wait a moment.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Output: Погодите минутку\n",
      "----------------------------------------\n",
      "Input:  Nobody slept.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Output: Никто не шелохнулся\n",
      "----------------------------------------\n",
      "Input:  I warned you.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Output: Я тебя предупреждал\n",
      "----------------------------------------\n",
      "Input:  I'll call Tom.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Output: Я позову Тома\n",
      "----------------------------------------\n",
      "Input:  Let me go!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Output: Отпусти меня\n",
      "----------------------------------------\n",
      "Input:  I like picnics.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Output: Мне нравится маджонг\n",
      "----------------------------------------\n",
      "Input:  We won today.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Output: Мы сегодня победили\n",
      "----------------------------------------\n",
      "Input:  Ignore that.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Output: Не обращайте на это внимания\n",
      "----------------------------------------\n",
      "Input:  Let's not go.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Output: Давайте не пойдём\n",
      "----------------------------------------\n",
      "Input:  He is falling.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Output: Он умирает\n",
      "----------------------------------------\n",
      "Input:  Don't be cruel.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Output: Не будьте жестокой\n",
      "----------------------------------------\n",
      "Input:  He is happy.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Output: Он герой\n",
      "----------------------------------------\n",
      "Input:  We can go now.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Output: Мы можем обязательно проверить\n",
      "----------------------------------------\n",
      "Input:  You'll do it.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Output: Вы это сделаете\n",
      "----------------------------------------\n",
      "Input:  Tom needed us.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Output: Том в нас нуждался\n",
      "----------------------------------------\n",
      "Input:  Cut it out!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Output: Прекрати\n",
      "----------------------------------------\n",
      "Input:  Do you sing?\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Output: Вы пьёте\n",
      "----------------------------------------\n",
      "Input:  I wasn't lucky.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Output: Мне не везло\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Создаем словарь для обратного поиска слова по индексу для русского\n",
    "target_vocab = target_vectorization.get_vocabulary()\n",
    "target_index_to_word = dict(enumerate(target_vocab))\n",
    "\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    # Преобразуем вход в вектор токенов\n",
    "    tokenized_input = source_vectorization([input_sentence])\n",
    "\n",
    "    # Начинаем со стартового токена [start]\n",
    "    decoded_sentence = \"[start]\"\n",
    "    \n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        # Преобразуем текущую часть предложения-декодера\n",
    "        tokenized_target = target_vectorization([decoded_sentence])\n",
    "        \n",
    "        # Предсказываем вероятности следующего слова\n",
    "        predictions = model.predict([tokenized_input, tokenized_target])\n",
    "        \n",
    "        # Выбираем токен с наибольшей вероятностью для текущего шага i\n",
    "        next_token_idx = np.argmax(predictions[0, i, :])\n",
    "        next_word = target_index_to_word[next_token_idx]\n",
    "        \n",
    "        # Добавляем токен к ответу\n",
    "        decoded_sentence += \" \" + next_word\n",
    "        \n",
    "        # Если достигли токена конца предложения, прерываем\n",
    "        if next_word == \"[end]\":\n",
    "            break\n",
    "\n",
    "    # Возвращаем срез без стартового и конечного токена\n",
    "    decoded_words = decoded_sentence.split()[1:-1]\n",
    "    return \" \".join(decoded_words)\n",
    "\n",
    "# Пример применения:\n",
    "test_english_sentences = [pair[0] for pair in test_pairs]\n",
    "\n",
    "import random\n",
    "\n",
    "for _ in range(20):\n",
    "    inp = random.choice(test_english_sentences)\n",
    "    print(\"Input: \", inp)\n",
    "    print(\"Output:\", decode_sequence(inp))\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368b498b",
   "metadata": {},
   "source": [
    "Как видно из сравнения, модель Transformer показывает лучшие результаты: она лучше справляется с контекстуальными особенностями перевода и даёт более точный перевод в сложных случаях.\n",
    "Время обучения отличается достаточно сильно — примерно на 45 процентов в пользу RNN. Так, среднее время обучения RNN составляет 3,11 минуты, а время обучения Transformer — 4,52 минуты.\n",
    "В заключении хотелось бы сказать, что трансформер работает намного лучше, чем RNN, в задаче машинного перевода. Это связано с весомыми преимуществами архитектуры трансформера, которые \n",
    "позволяют ему эффективно обрабатывать сложные зависимости и длинные контексты.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af8e55af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
