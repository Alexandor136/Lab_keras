{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b317091a",
   "metadata": {},
   "source": [
    "\n",
    "Контрольные вопросы:\n",
    "1) В чём основная идея архитектуры трансформеров и чем она отличается от RNN/LSTM?\n",
    "\n",
    "Основная идея трансформеров — это обработка всего входного текста (например, предложения) сразу, а не по частям последовательно, как в RNN/LSTM. Трансформер смотрит на все слова сразу и сам выбирает, какие слова важны для обработки каждого конкретного слова, используя механизм внимания (Attention).\n",
    "\n",
    "Главные отличия от RNN/LSTM:\n",
    "- Параллельность: Трансформеры обрабатывают все слова одновременно, что ускоряет обучение и позволяет работать с длинными текстами.\n",
    "- Отсутствие рекуррентности: RNN/LSTM работают по шагам (один за другим), что сложнее и медленнее.\n",
    "- Better long-range dependencies: Трансформеры легче улавливают связи между далекими словами, поскольку могут напрямую связать любое слово с любым другим.\n",
    "\n",
    "2) Из каких основных блоков состоит трансформер (назовите ключевые компоненты)?\n",
    "\n",
    "- Эмбеддинги (Embeddings): переводят слова в числовые векторы.\n",
    "- Позиционные эмбеддинги (Positional embeddings): добавляют информацию о положении слова в последовательности, чтобы модель знала порядок слов.\n",
    "- Self-Attention (Механизм само-внимания): позволяет модели увидеть, какие другие слова важны для каждого слова.\n",
    "- Feed-forward сети (FFN): небольшие нейронные сети для «обработки» признаков после внимания.\n",
    "- Нормализация и резидуальные связи (Layer norm + residual connections): улучшают стабильность и обучение.\n",
    "- Множественные головы внимания (Multi-head Attention): позволяют модели смотреть на разные аспекты текста одновременно.\n",
    "- Encoder и decoder: архитектура трансформера обычно состоит из encoder (для кодирования входа) и decoder (для генерации выхода), особенно в задачах перевода.\n",
    "\n",
    "3) Для чего нужен механизм Attention и какие преимущества он даёт по сравнению с рекуррентными сетями?\n",
    "\n",
    "Механизм Attention помогает модели сосредоточиться на наиболее важной части входных данных при обработке информации. \n",
    "Например, при переводе слова «банк» модель поймёт, что для правильного перевода важно учитывать контекст («река» или «деньги»).\n",
    "\n",
    "Преимущества перед RNN:\n",
    "\n",
    "Прямой доступ к любому слову последовательности: можно непосредственно учитывать важные слова, даже если они далеко.\n",
    "Параллельная обработка: быстрее обучение и вычисления.\n",
    "Лучшее понимание контекста: attention выделяет веса важности для каждого слова.\n",
    "\n",
    "4) Какие задачи решают BERT и GPT, в чём их принципиальное отличие?\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers):\n",
    "\n",
    "- Задача: понимание языка (например, классификация текста, ответ на вопросы, определение сущностей).\n",
    "Особенность: двунаправленность, то есть модель учитывает контекст как слева, так и справа от слова.\n",
    "Обучается на задачах заполнения пропусков в тексте (Masked Language Modeling) и предсказания следующего предложения.\n",
    "GPT (Generative Pre-trained Transformer):\n",
    "\n",
    "- Задача: генерация текста (например, создание связных абзацев, продолжение текста).\n",
    "Особенность: однонаправленный — тексты генерируются слева направо, используя только предыдущий контекст.\n",
    "Обучается на задаче предсказания следующего слова (Language Modeling).\n",
    "\n",
    "BERT больше для понимания существующего текста, GPT — для генерации нового.\n",
    "\n",
    "5) Что такое fine-tuning предобученной модели и почему он эффективен?\n",
    "\n",
    "Fine-tuning — это когда мы берём уже обученную модель (например, BERT или GPT), которая знает много о языке, и дообучаем её на конкретной задаче (например, категоризация отзывов или анализ тональности).\n",
    "\n",
    "Почему это эффективно?\n",
    "\n",
    "- Модель уже научилась понимать язык, что экономит время и данные.\n",
    "- Для конкретной задачи требуется немного обучения, так как базовое понимание уже есть.\n",
    "- Повышается качество на целевой задаче без необходимости строить модель с нуля.\n",
    "\n",
    "6) Как происходит токенизация в BERT (WordPiece)?\n",
    "\n",
    "WordPiece токенизация разбивает слова на более мелкие части (субслова). Например, слово «playing» может разбиться на «play» + «##ing».\n",
    "\n",
    "Если целое слово есть в словаре — используем его.\n",
    "Если нет — разрезаем на части так, чтобы подчасти были известны модели.\n",
    "Преимущество: модель может обработать редкие или новые слова, используя знакомые подслова.\n",
    "\n",
    "7) Какие метрики применяются для оценки качества в задаче бинарной классификации текстов?\n",
    "\n",
    "- Accuracy (Точность): доля правильно классифицированных примеров.\n",
    "- Precision (Точность предсказаний положительного класса): из всех положительных предсказаний, сколько правильных.\n",
    "- Recall (Полнота): сколько из всех реальных положительных примеров модель нашла.\n",
    "- F1-score: гармоническое среднее Precision и Recall — баланс между ними.\n",
    "- ROC-AUC: площадь под ROC-кривой — мера качества при разных порогах классификации.\n",
    "\n",
    "8) Как проверить, «понимает» ли модель содержание текста, а не просто запоминает частые шаблоны?\n",
    "\n",
    "1) Тестирование на новых данных с изменённым контекстом: если модель хорошо работает на примерах, отличающихся от обучающих.\n",
    "2) Интерпретируемость модели: анализируем важность слов (например, с помощью attention или LIME/SHAP).\n",
    "3) Adversarial examples: подаём чуть изменённые, но логичные тексты — если модель реагирует разумно, она действительно “понимает”.\n",
    "4) Cross-domain evaluation: проверяем модель на данных из другой области.\n",
    "Анализ ошибок: смотрим, где модель ошибается — если ошибки связаны с неверным контекстом, возможно, запоминание шаблонов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296b2700",
   "metadata": {},
   "source": [
    "Практические задания:\n",
    "1) Увеличить размер выборки.\n",
    "2) Сравнить с другой моделью.\n",
    "3) Изменить гиперпараметры модели.\n",
    "4) Обучить модели на русскоязычном наборе данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08558fe",
   "metadata": {},
   "source": [
    "#1. Увеличение размера выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac22c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aa-sablin/env_lab/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-02 16:32:35.435172: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748856755.450263    9315 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748856755.454412    9315 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748856755.467017    9315 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748856755.467039    9315 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748856755.467040    9315 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748856755.467041    9315 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-02 16:32:35.470752: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1406' max='1960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1406/1960 45:40 < 18:01, 0.51 it/s, Epoch 7.17/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.313600</td>\n",
       "      <td>0.275780</td>\n",
       "      <td>0.881960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.212900</td>\n",
       "      <td>0.278810</td>\n",
       "      <td>0.888000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.397376</td>\n",
       "      <td>0.884680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.445127</td>\n",
       "      <td>0.884280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.031900</td>\n",
       "      <td>0.538662</td>\n",
       "      <td>0.879800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.615352</td>\n",
       "      <td>0.881800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>0.648681</td>\n",
       "      <td>0.886480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "# 1. Загрузка набора данных IMDB (используем поднабор для демонстрации)\n",
    "dataset = load_dataset(\"imdb\")\n",
    "small_train_dataset = dataset[\"train\"].shuffle(seed=42)\n",
    "small_test_dataset = dataset[\"test\"].shuffle(seed=42)\n",
    "\n",
    "# 2. Загрузка токенизатора\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Функция токенизации\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "small_train_dataset = small_train_dataset.map(tokenize, batched=True)\n",
    "small_test_dataset = small_test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Убираем лишние колонки\n",
    "small_train_dataset = small_train_dataset.remove_columns([\"text\"])\n",
    "small_test_dataset = small_test_dataset.remove_columns([\"text\"])\n",
    "\n",
    "# 3. Загрузка модели для классификации\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# 4. Настройка параметров обучения\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=10,             \n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Метрика для оценки\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# 5. Создание Trainer и запуск обучения\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Запуск дообучения\n",
    "trainer.train()\n",
    "\n",
    "# Оценка модели\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d20e036",
   "metadata": {},
   "source": [
    "При увеличении выборки можем заметить, что после второй эпохи наблюдается переобучение, так как потери на тренировочной выборке падают, а на валидационной — растут."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de93d93",
   "metadata": {},
   "source": [
    "2) Сравнить с другой моделью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1f0d9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aa-sablin/env_lab/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-02 17:54:07.041876: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748861647.056563    6646 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748861647.060665    6646 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748861647.071114    6646 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748861647.071149    6646 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748861647.071150    6646 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748861647.071152    6646 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-02 17:54:07.074816: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='553' max='553' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [553/553 03:37, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.485500</td>\n",
       "      <td>0.460566</td>\n",
       "      <td>0.792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.285300</td>\n",
       "      <td>0.400194</td>\n",
       "      <td>0.838000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.172900</td>\n",
       "      <td>0.478162</td>\n",
       "      <td>0.837000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.071000</td>\n",
       "      <td>0.590374</td>\n",
       "      <td>0.834000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.042900</td>\n",
       "      <td>0.674269</td>\n",
       "      <td>0.838000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>0.746779</td>\n",
       "      <td>0.841000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>0.762634</td>\n",
       "      <td>0.841000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.7626335620880127, 'eval_accuracy': 0.841, 'eval_runtime': 1.9357, 'eval_samples_per_second': 516.618, 'eval_steps_per_second': 8.266, 'epoch': 7.0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments, DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "\n",
    "# 1. Загрузка набора данных IMDB (используем поднабор для демонстрации)\n",
    "dataset = load_dataset(\"imdb\")\n",
    "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(5000))\n",
    "small_test_dataset = dataset[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# 2. Загрузка токенизатора\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Функция токенизации\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "small_train_dataset = small_train_dataset.map(tokenize, batched=True)\n",
    "small_test_dataset = small_test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Убираем лишние колонки\n",
    "small_train_dataset = small_train_dataset.remove_columns([\"text\"])\n",
    "small_test_dataset = small_test_dataset.remove_columns([\"text\"])\n",
    "\n",
    "# 3. Загрузка модели для классификации\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# 4. Настройка параметров обучения\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=7,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Метрика для оценки\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# 5. Создание Trainer и запуск обучения\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Запуск дообучения\n",
    "trainer.train()\n",
    "\n",
    "# Оценка модели\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e490b3b",
   "metadata": {},
   "source": [
    "DistilBERT демонстрирует оптимальное соотношение скорость-ресурсы (3.5 мин обучения, 84.1% точности), \n",
    "превосходя BERT-base по скорости в 4× при потреблении в 3× меньше памяти. BERT обеспечивает высокую точность (88.7%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbf48e3",
   "metadata": {},
   "source": [
    "3) Изменить гиперпараметры модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65a9c8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [01:16<00:00, 327.61 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [01:15<00:00, 330.29 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [02:39<00:00, 314.25 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;34m||| LR=2.0e-05 | Batch=24 | Epochs=3 \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4398, 'grad_norm': 8.28321647644043, 'learning_rate': 1.341317365269461e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 0.3193889856338501, 'eval_accuracy': 0.85875, 'eval_f1': 0.8587232898719914, 'eval_runtime': 1.0058, 'eval_samples_per_second': 795.387, 'eval_steps_per_second': 16.902, 'epoch': 1.0}\n",
      "{'loss': 0.2398, 'grad_norm': 6.134963512420654, 'learning_rate': 6.746506986027944e-06, 'epoch': 2.0}\n",
      "{'eval_loss': 0.3071049451828003, 'eval_accuracy': 0.87125, 'eval_f1': 0.8712497988278107, 'eval_runtime': 1.0113, 'eval_samples_per_second': 791.052, 'eval_steps_per_second': 16.81, 'epoch': 2.0}\n",
      "{'loss': 0.1619, 'grad_norm': 9.876880645751953, 'learning_rate': 7.984031936127744e-08, 'epoch': 3.0}\n",
      "{'eval_loss': 0.3346048593521118, 'eval_accuracy': 0.87625, 'eval_f1': 0.8762173136343816, 'eval_runtime': 1.0214, 'eval_samples_per_second': 783.231, 'eval_steps_per_second': 16.644, 'epoch': 3.0}\n",
      "{'train_runtime': 58.431, 'train_samples_per_second': 205.371, 'train_steps_per_second': 8.574, 'train_loss': 0.2804935059385623, 'epoch': 3.0}\n",
      "{'eval_loss': 0.3346048593521118, 'eval_accuracy': 0.87625, 'eval_f1': 0.8762173136343816, 'eval_runtime': 1.0213, 'eval_samples_per_second': 783.28, 'eval_steps_per_second': 16.645, 'epoch': 3.0}\n",
      "► Accuracy: 0.8762 | Memory: 3308MB\n",
      "\n",
      "\u001b[1;34m||| LR=2.0e-05 | Batch=24 | Epochs=4 \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4414, 'grad_norm': 7.139427661895752, 'learning_rate': 1.5059880239520958e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 0.3162311911582947, 'eval_accuracy': 0.8575, 'eval_f1': 0.8574777308954524, 'eval_runtime': 1.0034, 'eval_samples_per_second': 797.282, 'eval_steps_per_second': 16.942, 'epoch': 1.0}\n",
      "{'loss': 0.2379, 'grad_norm': 8.262118339538574, 'learning_rate': 1.0059880239520958e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 0.33282092213630676, 'eval_accuracy': 0.8675, 'eval_f1': 0.8673134094820842, 'eval_runtime': 1.0098, 'eval_samples_per_second': 792.233, 'eval_steps_per_second': 16.835, 'epoch': 2.0}\n",
      "{'loss': 0.1535, 'grad_norm': 5.680226802825928, 'learning_rate': 5.0898203592814375e-06, 'epoch': 3.0}\n",
      "{'eval_loss': 0.3611493706703186, 'eval_accuracy': 0.87875, 'eval_f1': 0.8785433338915749, 'eval_runtime': 1.0055, 'eval_samples_per_second': 795.599, 'eval_steps_per_second': 16.906, 'epoch': 3.0}\n",
      "{'loss': 0.0993, 'grad_norm': 17.258590698242188, 'learning_rate': 8.982035928143712e-08, 'epoch': 4.0}\n",
      "{'eval_loss': 0.4067394733428955, 'eval_accuracy': 0.87, 'eval_f1': 0.8699991874949218, 'eval_runtime': 1.0063, 'eval_samples_per_second': 794.979, 'eval_steps_per_second': 16.893, 'epoch': 4.0}\n",
      "{'train_runtime': 77.7129, 'train_samples_per_second': 205.886, 'train_steps_per_second': 8.596, 'train_loss': 0.2330219417275069, 'epoch': 4.0}\n",
      "{'eval_loss': 0.4067394733428955, 'eval_accuracy': 0.87, 'eval_f1': 0.8699991874949218, 'eval_runtime': 1.0081, 'eval_samples_per_second': 793.535, 'eval_steps_per_second': 16.863, 'epoch': 4.0}\n",
      "► Accuracy: 0.8700 | Memory: 3308MB\n",
      "\n",
      "\u001b[1;34m||| LR=2.0e-05 | Batch=32 | Epochs=3 \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4418, 'grad_norm': 4.882176876068115, 'learning_rate': 1.3440000000000002e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 0.37021881341934204, 'eval_accuracy': 0.83875, 'eval_f1': 0.8378069997595394, 'eval_runtime': 0.9974, 'eval_samples_per_second': 802.059, 'eval_steps_per_second': 13.033, 'epoch': 1.0}\n",
      "{'loss': 0.2566, 'grad_norm': 8.942131042480469, 'learning_rate': 6.773333333333334e-06, 'epoch': 2.0}\n",
      "{'eval_loss': 0.3185597360134125, 'eval_accuracy': 0.87375, 'eval_f1': 0.873716653303763, 'eval_runtime': 0.9992, 'eval_samples_per_second': 800.64, 'eval_steps_per_second': 13.01, 'epoch': 2.0}\n",
      "{'loss': 0.1768, 'grad_norm': 5.86332893371582, 'learning_rate': 1.0666666666666667e-07, 'epoch': 3.0}\n",
      "{'eval_loss': 0.34154289960861206, 'eval_accuracy': 0.86625, 'eval_f1': 0.8662481191141751, 'eval_runtime': 0.999, 'eval_samples_per_second': 800.839, 'eval_steps_per_second': 13.014, 'epoch': 3.0}\n",
      "{'train_runtime': 55.4455, 'train_samples_per_second': 216.429, 'train_steps_per_second': 6.763, 'train_loss': 0.29171583048502603, 'epoch': 3.0}\n",
      "{'eval_loss': 0.34154289960861206, 'eval_accuracy': 0.86625, 'eval_f1': 0.8662481191141751, 'eval_runtime': 0.9973, 'eval_samples_per_second': 802.158, 'eval_steps_per_second': 13.035, 'epoch': 3.0}\n",
      "► Accuracy: 0.8662 | Memory: 3308MB\n",
      "\n",
      "\u001b[1;34m||| LR=2.0e-05 | Batch=32 | Epochs=4 \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4582, 'grad_norm': 4.014491081237793, 'learning_rate': 1.5080000000000001e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 0.3636631667613983, 'eval_accuracy': 0.84125, 'eval_f1': 0.8403818267797191, 'eval_runtime': 1.003, 'eval_samples_per_second': 797.638, 'eval_steps_per_second': 12.962, 'epoch': 1.0}\n",
      "{'loss': 0.2486, 'grad_norm': 5.254928112030029, 'learning_rate': 1.008e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 0.33399537205696106, 'eval_accuracy': 0.86125, 'eval_f1': 0.8611143695014662, 'eval_runtime': 0.998, 'eval_samples_per_second': 801.57, 'eval_steps_per_second': 13.026, 'epoch': 2.0}\n",
      "{'loss': 0.1705, 'grad_norm': 10.44433307647705, 'learning_rate': 5.0800000000000005e-06, 'epoch': 3.0}\n",
      "{'eval_loss': 0.3487713634967804, 'eval_accuracy': 0.86375, 'eval_f1': 0.8637242353632484, 'eval_runtime': 0.9992, 'eval_samples_per_second': 800.663, 'eval_steps_per_second': 13.011, 'epoch': 3.0}\n",
      "{'loss': 0.1159, 'grad_norm': 6.2729926109313965, 'learning_rate': 8e-08, 'epoch': 4.0}\n",
      "{'eval_loss': 0.36902886629104614, 'eval_accuracy': 0.865, 'eval_f1': 0.8649991562447266, 'eval_runtime': 1.0117, 'eval_samples_per_second': 790.782, 'eval_steps_per_second': 12.85, 'epoch': 4.0}\n",
      "{'train_runtime': 73.9161, 'train_samples_per_second': 216.462, 'train_steps_per_second': 6.764, 'train_loss': 0.24830754470825195, 'epoch': 4.0}\n",
      "{'eval_loss': 0.36902886629104614, 'eval_accuracy': 0.865, 'eval_f1': 0.8649991562447266, 'eval_runtime': 1.0069, 'eval_samples_per_second': 794.525, 'eval_steps_per_second': 12.911, 'epoch': 4.0}\n",
      "► Accuracy: 0.8650 | Memory: 3308MB\n",
      "\n",
      "\u001b[1;34m||| LR=3.0e-05 | Batch=24 | Epochs=3 \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4166, 'grad_norm': 11.150524139404297, 'learning_rate': 2.0119760479041917e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 0.32722190022468567, 'eval_accuracy': 0.8525, 'eval_f1': 0.8521664505540626, 'eval_runtime': 1.0159, 'eval_samples_per_second': 787.46, 'eval_steps_per_second': 16.734, 'epoch': 1.0}\n",
      "{'loss': 0.2144, 'grad_norm': 7.79978084564209, 'learning_rate': 1.0179640718562873e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 0.36196762323379517, 'eval_accuracy': 0.87125, 'eval_f1': 0.8711031784642194, 'eval_runtime': 1.0287, 'eval_samples_per_second': 777.689, 'eval_steps_per_second': 16.526, 'epoch': 2.0}\n",
      "{'loss': 0.1229, 'grad_norm': 6.063252925872803, 'learning_rate': 1.7964071856287427e-07, 'epoch': 3.0}\n",
      "{'eval_loss': 0.40438881516456604, 'eval_accuracy': 0.865, 'eval_f1': 0.8649966249156229, 'eval_runtime': 1.0399, 'eval_samples_per_second': 769.287, 'eval_steps_per_second': 16.347, 'epoch': 3.0}\n",
      "{'train_runtime': 58.5318, 'train_samples_per_second': 205.017, 'train_steps_per_second': 8.559, 'train_loss': 0.25129555037873474, 'epoch': 3.0}\n",
      "{'eval_loss': 0.40438881516456604, 'eval_accuracy': 0.865, 'eval_f1': 0.8649966249156229, 'eval_runtime': 1.0074, 'eval_samples_per_second': 794.154, 'eval_steps_per_second': 16.876, 'epoch': 3.0}\n",
      "► Accuracy: 0.8650 | Memory: 3308MB\n",
      "\n",
      "\u001b[1;34m||| LR=3.0e-05 | Batch=24 | Epochs=4 \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4264, 'grad_norm': 6.125901222229004, 'learning_rate': 2.2589820359281435e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 0.3075236976146698, 'eval_accuracy': 0.8625, 'eval_f1': 0.8624862486248626, 'eval_runtime': 1.0067, 'eval_samples_per_second': 794.711, 'eval_steps_per_second': 16.888, 'epoch': 1.0}\n",
      "{'loss': 0.2214, 'grad_norm': 11.422225952148438, 'learning_rate': 1.5089820359281436e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 0.3654230535030365, 'eval_accuracy': 0.86875, 'eval_f1': 0.8685773023613843, 'eval_runtime': 1.0065, 'eval_samples_per_second': 794.81, 'eval_steps_per_second': 16.89, 'epoch': 2.0}\n",
      "{'loss': 0.1249, 'grad_norm': 13.275642395019531, 'learning_rate': 7.634730538922157e-06, 'epoch': 3.0}\n",
      "{'eval_loss': 0.39736407995224, 'eval_accuracy': 0.88, 'eval_f1': 0.8797565069265262, 'eval_runtime': 1.0068, 'eval_samples_per_second': 794.557, 'eval_steps_per_second': 16.884, 'epoch': 3.0}\n",
      "{'loss': 0.0639, 'grad_norm': 27.2251033782959, 'learning_rate': 1.347305389221557e-07, 'epoch': 4.0}\n",
      "{'eval_loss': 0.47721976041793823, 'eval_accuracy': 0.86875, 'eval_f1': 0.8687497949215546, 'eval_runtime': 1.0073, 'eval_samples_per_second': 794.209, 'eval_steps_per_second': 16.877, 'epoch': 4.0}\n",
      "{'train_runtime': 77.8434, 'train_samples_per_second': 205.541, 'train_steps_per_second': 8.581, 'train_loss': 0.20913056699101795, 'epoch': 4.0}\n",
      "{'eval_loss': 0.47721976041793823, 'eval_accuracy': 0.86875, 'eval_f1': 0.8687497949215546, 'eval_runtime': 1.0235, 'eval_samples_per_second': 781.606, 'eval_steps_per_second': 16.609, 'epoch': 4.0}\n",
      "► Accuracy: 0.8688 | Memory: 3308MB\n",
      "\n",
      "\u001b[1;34m||| LR=3.0e-05 | Batch=32 | Epochs=3 \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4304, 'grad_norm': 4.959465980529785, 'learning_rate': 2.016e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 0.35503944754600525, 'eval_accuracy': 0.84375, 'eval_f1': 0.8429527397679784, 'eval_runtime': 1.0336, 'eval_samples_per_second': 773.982, 'eval_steps_per_second': 12.577, 'epoch': 1.0}\n",
      "{'loss': 0.2303, 'grad_norm': 5.35066556930542, 'learning_rate': 1.0160000000000001e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 0.32905226945877075, 'eval_accuracy': 0.86875, 'eval_f1': 0.8687481542709194, 'eval_runtime': 1.0192, 'eval_samples_per_second': 784.966, 'eval_steps_per_second': 12.756, 'epoch': 2.0}\n",
      "{'loss': 0.1369, 'grad_norm': 4.218013286590576, 'learning_rate': 1.6e-07, 'epoch': 3.0}\n",
      "{'eval_loss': 0.3583962321281433, 'eval_accuracy': 0.865, 'eval_f1': 0.8649586435845977, 'eval_runtime': 1.0226, 'eval_samples_per_second': 782.346, 'eval_steps_per_second': 12.713, 'epoch': 3.0}\n",
      "{'train_runtime': 55.4453, 'train_samples_per_second': 216.429, 'train_steps_per_second': 6.763, 'train_loss': 0.26584647623697916, 'epoch': 3.0}\n",
      "{'eval_loss': 0.3583962321281433, 'eval_accuracy': 0.865, 'eval_f1': 0.8649586435845977, 'eval_runtime': 1.0328, 'eval_samples_per_second': 774.628, 'eval_steps_per_second': 12.588, 'epoch': 3.0}\n",
      "► Accuracy: 0.8650 | Memory: 3308MB\n",
      "\n",
      "\u001b[1;34m||| LR=3.0e-05 | Batch=32 | Epochs=4 \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4467, 'grad_norm': 4.346563339233398, 'learning_rate': 2.262e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 0.3548020124435425, 'eval_accuracy': 0.83625, 'eval_f1': 0.8353544827412851, 'eval_runtime': 1.0079, 'eval_samples_per_second': 793.748, 'eval_steps_per_second': 12.898, 'epoch': 1.0}\n",
      "{'loss': 0.2282, 'grad_norm': 3.753711462020874, 'learning_rate': 1.5120000000000001e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 0.3418346643447876, 'eval_accuracy': 0.8625, 'eval_f1': 0.8624785122675418, 'eval_runtime': 1.0281, 'eval_samples_per_second': 778.099, 'eval_steps_per_second': 12.644, 'epoch': 2.0}\n",
      "{'loss': 0.1349, 'grad_norm': 7.135671138763428, 'learning_rate': 7.62e-06, 'epoch': 3.0}\n",
      "{'eval_loss': 0.3498421609401703, 'eval_accuracy': 0.875, 'eval_f1': 0.8748678291445339, 'eval_runtime': 1.019, 'eval_samples_per_second': 785.1, 'eval_steps_per_second': 12.758, 'epoch': 3.0}\n",
      "{'loss': 0.0776, 'grad_norm': 4.697340965270996, 'learning_rate': 1.2000000000000002e-07, 'epoch': 4.0}\n",
      "{'eval_loss': 0.40726688504219055, 'eval_accuracy': 0.875, 'eval_f1': 0.8749804656977653, 'eval_runtime': 1.0122, 'eval_samples_per_second': 790.373, 'eval_steps_per_second': 12.844, 'epoch': 4.0}\n",
      "{'train_runtime': 74.0062, 'train_samples_per_second': 216.198, 'train_steps_per_second': 6.756, 'train_loss': 0.2218703670501709, 'epoch': 4.0}\n",
      "{'eval_loss': 0.40726688504219055, 'eval_accuracy': 0.875, 'eval_f1': 0.8749804656977653, 'eval_runtime': 1.0024, 'eval_samples_per_second': 798.046, 'eval_steps_per_second': 12.968, 'epoch': 4.0}\n",
      "► Accuracy: 0.8750 | Memory: 3308MB\n",
      "\n",
      "\u001b[1;34m||| LR=5.0e-05 | Batch=24 | Epochs=3 \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4172, 'grad_norm': 9.662147521972656, 'learning_rate': 3.3532934131736525e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 0.32959669828414917, 'eval_accuracy': 0.84875, 'eval_f1': 0.8482699165327795, 'eval_runtime': 1.0062, 'eval_samples_per_second': 795.035, 'eval_steps_per_second': 16.894, 'epoch': 1.0}\n",
      "{'loss': 0.1884, 'grad_norm': 2.684006929397583, 'learning_rate': 1.6966067864271457e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 0.44730690121650696, 'eval_accuracy': 0.86375, 'eval_f1': 0.8636372876956109, 'eval_runtime': 1.0195, 'eval_samples_per_second': 784.729, 'eval_steps_per_second': 16.675, 'epoch': 2.0}\n",
      "{'loss': 0.0832, 'grad_norm': 4.819431781768799, 'learning_rate': 2.9940119760479047e-07, 'epoch': 3.0}\n",
      "{'eval_loss': 0.523221492767334, 'eval_accuracy': 0.86875, 'eval_f1': 0.8687497949215546, 'eval_runtime': 1.002, 'eval_samples_per_second': 798.403, 'eval_steps_per_second': 16.966, 'epoch': 3.0}\n",
      "{'train_runtime': 58.4433, 'train_samples_per_second': 205.327, 'train_steps_per_second': 8.572, 'train_loss': 0.2296209468575057, 'epoch': 3.0}\n",
      "{'eval_loss': 0.523221492767334, 'eval_accuracy': 0.86875, 'eval_f1': 0.8687497949215546, 'eval_runtime': 0.9992, 'eval_samples_per_second': 800.678, 'eval_steps_per_second': 17.014, 'epoch': 3.0}\n",
      "► Accuracy: 0.8688 | Memory: 3308MB\n",
      "\n",
      "\u001b[1;34m||| LR=5.0e-05 | Batch=24 | Epochs=4 \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4133, 'grad_norm': 5.330784320831299, 'learning_rate': 3.7724550898203595e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 0.3450813591480255, 'eval_accuracy': 0.8425, 'eval_f1': 0.8417244498040398, 'eval_runtime': 1.029, 'eval_samples_per_second': 777.466, 'eval_steps_per_second': 16.521, 'epoch': 1.0}\n",
      "{'loss': 0.2088, 'grad_norm': 13.03296184539795, 'learning_rate': 2.5224550898203592e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 0.3902049958705902, 'eval_accuracy': 0.86375, 'eval_f1': 0.8637395675606414, 'eval_runtime': 1.0436, 'eval_samples_per_second': 766.587, 'eval_steps_per_second': 16.29, 'epoch': 2.0}\n",
      "{'loss': 0.0992, 'grad_norm': 21.583200454711914, 'learning_rate': 1.2724550898203593e-05, 'epoch': 3.0}\n",
      "{'eval_loss': 0.4874677360057831, 'eval_accuracy': 0.86375, 'eval_f1': 0.8637480839574306, 'eval_runtime': 1.0011, 'eval_samples_per_second': 799.16, 'eval_steps_per_second': 16.982, 'epoch': 3.0}\n",
      "{'loss': 0.0414, 'grad_norm': 0.05090377852320671, 'learning_rate': 2.2455089820359282e-07, 'epoch': 4.0}\n",
      "{'eval_loss': 0.5809140801429749, 'eval_accuracy': 0.85375, 'eval_f1': 0.8537314878914363, 'eval_runtime': 0.9975, 'eval_samples_per_second': 802.014, 'eval_steps_per_second': 17.043, 'epoch': 4.0}\n",
      "{'train_runtime': 77.736, 'train_samples_per_second': 205.825, 'train_steps_per_second': 8.593, 'train_loss': 0.19066974074540738, 'epoch': 4.0}\n",
      "{'eval_loss': 0.5809140801429749, 'eval_accuracy': 0.85375, 'eval_f1': 0.8537314878914363, 'eval_runtime': 0.9982, 'eval_samples_per_second': 801.425, 'eval_steps_per_second': 17.03, 'epoch': 4.0}\n",
      "► Accuracy: 0.8538 | Memory: 3308MB\n",
      "\n",
      "\u001b[1;34m||| LR=5.0e-05 | Batch=32 | Epochs=3 \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.411, 'grad_norm': 3.5291192531585693, 'learning_rate': 3.3600000000000004e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 0.31074029207229614, 'eval_accuracy': 0.855, 'eval_f1': 0.8547376198258103, 'eval_runtime': 0.9985, 'eval_samples_per_second': 801.208, 'eval_steps_per_second': 13.02, 'epoch': 1.0}\n",
      "{'loss': 0.1893, 'grad_norm': 2.9332337379455566, 'learning_rate': 1.6933333333333333e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 0.34411078691482544, 'eval_accuracy': 0.86875, 'eval_f1': 0.8687481542709194, 'eval_runtime': 0.9582, 'eval_samples_per_second': 834.859, 'eval_steps_per_second': 13.566, 'epoch': 2.0}\n",
      "{'loss': 0.0937, 'grad_norm': 4.812600135803223, 'learning_rate': 2.6666666666666667e-07, 'epoch': 3.0}\n",
      "{'eval_loss': 0.4434668719768524, 'eval_accuracy': 0.86875, 'eval_f1': 0.8687399504024527, 'eval_runtime': 0.9563, 'eval_samples_per_second': 836.521, 'eval_steps_per_second': 13.593, 'epoch': 3.0}\n",
      "{'train_runtime': 54.1649, 'train_samples_per_second': 221.546, 'train_steps_per_second': 6.923, 'train_loss': 0.23134208424886069, 'epoch': 3.0}\n",
      "{'eval_loss': 0.4434668719768524, 'eval_accuracy': 0.86875, 'eval_f1': 0.8687399504024527, 'eval_runtime': 0.9577, 'eval_samples_per_second': 835.359, 'eval_steps_per_second': 13.575, 'epoch': 3.0}\n",
      "► Accuracy: 0.8688 | Memory: 3308MB\n",
      "\n",
      "\u001b[1;34m||| LR=5.0e-05 | Batch=32 | Epochs=4 \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.429, 'grad_norm': 3.9998083114624023, 'learning_rate': 3.77e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 0.35855910181999207, 'eval_accuracy': 0.83125, 'eval_f1': 0.8299102776714717, 'eval_runtime': 0.9581, 'eval_samples_per_second': 834.948, 'eval_steps_per_second': 13.568, 'epoch': 1.0}\n",
      "{'loss': 0.2051, 'grad_norm': 5.5343403816223145, 'learning_rate': 2.5200000000000003e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 0.33785584568977356, 'eval_accuracy': 0.8625, 'eval_f1': 0.8623959369273013, 'eval_runtime': 0.9577, 'eval_samples_per_second': 835.371, 'eval_steps_per_second': 13.575, 'epoch': 2.0}\n",
      "{'loss': 0.0949, 'grad_norm': 7.714051723480225, 'learning_rate': 1.27e-05, 'epoch': 3.0}\n",
      "{'eval_loss': 0.5267953276634216, 'eval_accuracy': 0.845, 'eval_f1': 0.8449844984498449, 'eval_runtime': 0.9576, 'eval_samples_per_second': 835.452, 'eval_steps_per_second': 13.576, 'epoch': 3.0}\n",
      "{'loss': 0.0419, 'grad_norm': 0.16962431371212006, 'learning_rate': 2.0000000000000002e-07, 'epoch': 4.0}\n",
      "{'eval_loss': 0.5569100975990295, 'eval_accuracy': 0.8525, 'eval_f1': 0.8524917026582746, 'eval_runtime': 0.9962, 'eval_samples_per_second': 803.037, 'eval_steps_per_second': 13.049, 'epoch': 4.0}\n",
      "{'train_runtime': 71.0863, 'train_samples_per_second': 225.079, 'train_steps_per_second': 7.034, 'train_loss': 0.1927294225692749, 'epoch': 4.0}\n",
      "{'eval_loss': 0.5569100975990295, 'eval_accuracy': 0.8525, 'eval_f1': 0.8524917026582746, 'eval_runtime': 0.9962, 'eval_samples_per_second': 803.053, 'eval_steps_per_second': 13.05, 'epoch': 4.0}\n",
      "► Accuracy: 0.8525 | Memory: 3308MB\n",
      "\n",
      "\u001b[1;92m==================================================\n",
      "⚡ Лучшая точность: 0.8762\n",
      "⚙️ Оптимальные параметры:\n",
      "   • Скорость обучения: 2.0e-05\n",
      "   • Размер батча: 24\n",
      "   • Эпохи: 3\n",
      "==================================================\u001b[0m\n",
      "\n",
      "Детальные результаты:\n",
      "Конфиг 1: LR=2.0e-05, BS=24, E=3 → Acc=0.8762\n",
      "Конфиг 2: LR=2.0e-05, BS=24, E=4 → Acc=0.8700\n",
      "Конфиг 3: LR=2.0e-05, BS=32, E=3 → Acc=0.8662\n",
      "Конфиг 4: LR=2.0e-05, BS=32, E=4 → Acc=0.8650\n",
      "Конфиг 5: LR=3.0e-05, BS=24, E=3 → Acc=0.8650\n",
      "Конфиг 6: LR=3.0e-05, BS=24, E=4 → Acc=0.8688\n",
      "Конфиг 7: LR=3.0e-05, BS=32, E=3 → Acc=0.8650\n",
      "Конфиг 8: LR=3.0e-05, BS=32, E=4 → Acc=0.8750\n",
      "Конфиг 9: LR=5.0e-05, BS=24, E=3 → Acc=0.8688\n",
      "Конфиг 10: LR=5.0e-05, BS=24, E=4 → Acc=0.8538\n",
      "Конфиг 11: LR=5.0e-05, BS=32, E=3 → Acc=0.8688\n",
      "Конфиг 12: LR=5.0e-05, BS=32, E=4 → Acc=0.8525\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Очистка памяти перед запуском\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Фиксируем генератор случайных чисел для воспроизводимости\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "# 1. Загрузка данных с буферизацией для ускорения доступа\n",
    "dataset = load_dataset('imdb', keep_in_memory=True)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# 2. Предварительная токенизация с батчингом\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(\n",
    "        batch['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=192,  # Сокращенный размер для ускорения\n",
    "        return_tensors='np'\n",
    "    )\n",
    "\n",
    "dataset = dataset.map(\n",
    "    tokenize_batch,\n",
    "    batched=True,\n",
    "    batch_size=128,\n",
    "    remove_columns=['text'],\n",
    "    keep_in_memory=True\n",
    ")\n",
    "\n",
    "# Выбор компактного подмножества\n",
    "train_dataset = dataset['train'].shuffle(seed=RANDOM_SEED).select(range(4000))\n",
    "val_dataset = dataset['test'].shuffle(seed=RANDOM_SEED).select(range(800))\n",
    "\n",
    "# 3. Функция метрик с буферизацией\n",
    "def compute_metrics(p):\n",
    "    predictions = np.argmax(p.predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(p.label_ids, predictions),\n",
    "        'f1': f1_score(p.label_ids, predictions, average='macro')\n",
    "    }\n",
    "\n",
    "# 4. Оптимизированный процесс поиска\n",
    "learning_rates = [2e-5, 3e-5, 5e-5]\n",
    "batch_sizes = [24, 32]\n",
    "epoch_options = [3, 4]\n",
    "\n",
    "results = []\n",
    "best_acc = 0.0\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        for epochs in epoch_options:\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            print(f\"\\n\\033[1;34m||| LR={lr:.1e} | Batch={bs} | Epochs={epochs} \\033[0m\")\n",
    "            \n",
    "            model = DistilBertForSequenceClassification.from_pretrained(\n",
    "                'distilbert-base-uncased',\n",
    "                num_labels=2\n",
    "            )\n",
    "            \n",
    "            args = TrainingArguments(\n",
    "                output_dir='./hparam_tuning',\n",
    "                learning_rate=float(lr),  # Явное преобразование во float\n",
    "                per_device_train_batch_size=bs,\n",
    "                per_device_eval_batch_size=bs*2,\n",
    "                num_train_epochs=epochs,\n",
    "                eval_strategy ='epoch',\n",
    "                logging_strategy='epoch',\n",
    "                weight_decay=0.01,\n",
    "                fp16=True,  # Обязательное использование ускорения на GPU\n",
    "                seed=RANDOM_SEED,\n",
    "                save_strategy='no',\n",
    "                optim='adamw_torch',\n",
    "                logging_steps=50,\n",
    "                disable_tqdm=True,\n",
    "                report_to='none',\n",
    "                log_level='warning'\n",
    "            )\n",
    "            \n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=val_dataset,\n",
    "                compute_metrics=compute_metrics\n",
    "            )\n",
    "            \n",
    "            trainer.train()\n",
    "            metrics = trainer.evaluate()\n",
    "            \n",
    "            current_acc = metrics['eval_accuracy']\n",
    "            results.append({\n",
    "                'lr': lr,\n",
    "                'batch_size': bs,\n",
    "                'epochs': epochs,\n",
    "                'accuracy': current_acc\n",
    "            })\n",
    "            \n",
    "            if current_acc > best_acc:\n",
    "                best_acc = current_acc\n",
    "                best_config = results[-1].copy()\n",
    "            \n",
    "            print(f\"► Accuracy: {current_acc:.4f} | \" \n",
    "                  f\"Memory: {torch.cuda.max_memory_allocated()/1024**2:.0f}MB\")\n",
    "\n",
    "# 5. Результаты\n",
    "print(\"\\n\\033[1;92m\" + \"=\"*50)\n",
    "print(f\"⚡ Лучшая точность: {best_acc:.4f}\")\n",
    "print(f\"⚙️ Оптимальные параметры:\")\n",
    "print(f\"   • Скорость обучения: {best_config['lr']:.1e}\")\n",
    "print(f\"   • Размер батча: {best_config['batch_size']}\")\n",
    "print(f\"   • Эпохи: {best_config['epochs']}\")\n",
    "print(\"=\"*50 + \"\\033[0m\")\n",
    "\n",
    "print(\"\\nДетальные результаты:\")\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"Конфиг {i+1}: LR={r['lr']:.1e}, BS={r['batch_size']}, E={r['epochs']}\"\n",
    "          f\" → Acc={r['accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae756a5",
   "metadata": {},
   "source": [
    "4) Обучить модели на русскоязычном наборе данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dada911d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.52.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c861f47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Должно вывести True, если GPU доступна\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca2b66c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review</th>\n",
       "      <th>translation</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Фильм сняли просто чудно. Disney на этот раз н...</td>\n",
       "      <td>The film was shot just wonderfully. Disney thi...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>На самом деле, к фильму у меня было предубежде...</td>\n",
       "      <td>In fact, I had a bias towards the film. I thou...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Пожалуй нет ни одного увлеченного киномана, кт...</td>\n",
       "      <td>Perhaps there is not a single keen film fan wh...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Я обожаю старый мультфильм 'Красавица и чудови...</td>\n",
       "      <td>I love the old Beauty and the Beast cartoon. L...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Никогда не фанател от «Форсажа», для меня он в...</td>\n",
       "      <td>I have never been a fan of Fast and the Furiou...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>2995</td>\n",
       "      <td>Когда в твоих руках философский камень, очень ...</td>\n",
       "      <td>When the philosopher's stone is in your hands,...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>2996</td>\n",
       "      <td>Фильм не плохой, но и не достаточно хороший. О...</td>\n",
       "      <td>The film is not bad, but not good enough eithe...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>2997</td>\n",
       "      <td>Вот что называется фильм продуман до мелочей! ...</td>\n",
       "      <td>That's what the film is called thought out to ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>2998</td>\n",
       "      <td>Знаете, когда речь заходит об этом фильме, то ...</td>\n",
       "      <td>You know, when it comes to this film, I immedi...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>2999</td>\n",
       "      <td>Благодаря рецензиям ряда пользователей 'Кинопо...</td>\n",
       "      <td>Thanks to the reviews of a number of users of ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                             review  \\\n",
       "0              0  Фильм сняли просто чудно. Disney на этот раз н...   \n",
       "1              1  На самом деле, к фильму у меня было предубежде...   \n",
       "2              2  Пожалуй нет ни одного увлеченного киномана, кт...   \n",
       "3              3  Я обожаю старый мультфильм 'Красавица и чудови...   \n",
       "4              4  Никогда не фанател от «Форсажа», для меня он в...   \n",
       "...          ...                                                ...   \n",
       "2995        2995  Когда в твоих руках философский камень, очень ...   \n",
       "2996        2996  Фильм не плохой, но и не достаточно хороший. О...   \n",
       "2997        2997  Вот что называется фильм продуман до мелочей! ...   \n",
       "2998        2998  Знаете, когда речь заходит об этом фильме, то ...   \n",
       "2999        2999  Благодаря рецензиям ряда пользователей 'Кинопо...   \n",
       "\n",
       "                                            translation     type  \n",
       "0     The film was shot just wonderfully. Disney thi...     good  \n",
       "1     In fact, I had a bias towards the film. I thou...     good  \n",
       "2     Perhaps there is not a single keen film fan wh...     good  \n",
       "3     I love the old Beauty and the Beast cartoon. L...     good  \n",
       "4     I have never been a fan of Fast and the Furiou...     good  \n",
       "...                                                 ...      ...  \n",
       "2995  When the philosopher's stone is in your hands,...  neutral  \n",
       "2996  The film is not bad, but not good enough eithe...  neutral  \n",
       "2997  That's what the film is called thought out to ...  neutral  \n",
       "2998  You know, when it comes to this film, I immedi...  neutral  \n",
       "2999  Thanks to the reviews of a number of users of ...  neutral  \n",
       "\n",
       "[3000 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"kt.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93fbbf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000 entries, 0 to 2999\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Unnamed: 0   3000 non-null   int64 \n",
      " 1   review       3000 non-null   object\n",
      " 2   translation  3000 non-null   object\n",
      " 3   type         3000 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 93.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['good', 'bad', 'neutral'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info()\n",
    "df[\"type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3538fa5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Распределение классов:\n",
      "type\n",
      "good       1001\n",
      "neutral    1000\n",
      "bad         999\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2100/2100 [00:00<00:00, 2612.81 examples/s]\n",
      "Map: 100%|██████████| 450/450 [00:00<00:00, 2680.63 examples/s]\n",
      "Map: 100%|██████████| 450/450 [00:00<00:00, 2575.92 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='704' max='704' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [704/704 04:38, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.115100</td>\n",
       "      <td>1.153080</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.095900</td>\n",
       "      <td>1.132064</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>0.268519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.083700</td>\n",
       "      <td>1.079103</td>\n",
       "      <td>0.395556</td>\n",
       "      <td>0.316116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.993100</td>\n",
       "      <td>1.017269</td>\n",
       "      <td>0.486667</td>\n",
       "      <td>0.485040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.881200</td>\n",
       "      <td>1.059133</td>\n",
       "      <td>0.524444</td>\n",
       "      <td>0.513339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.683800</td>\n",
       "      <td>1.100489</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.509847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.530100</td>\n",
       "      <td>1.195133</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.517175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.470600</td>\n",
       "      <td>1.242254</td>\n",
       "      <td>0.522222</td>\n",
       "      <td>0.516776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Оценка на тестовых данных...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "Accuracy: 0.4667\n",
      "F1 Score: 0.4637\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "\n",
    "# Преобразование меток в числовой формат\n",
    "label_map = {'good': 2, 'neutral': 1, 'bad': 0}\n",
    "df['label'] = df['type'].map(label_map).astype(int)\n",
    "\n",
    "# Проверка распределения классов\n",
    "print(\"Распределение классов:\")\n",
    "print(df['type'].value_counts())\n",
    "\n",
    "# Разделение на train/val/test\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['label'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n",
    "\n",
    "# Конвертация в формат Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df[['review', 'label']].reset_index(drop=True))\n",
    "val_dataset = Dataset.from_pandas(val_df[['review', 'label']].reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_df[['review', 'label']].reset_index(drop=True))\n",
    "\n",
    "# 2. Инициализация токенизатора\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Функция токенизации\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch['review'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "# Применение токенизации\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Удаление лишних колонок\n",
    "train_dataset = train_dataset.remove_columns(['review'])\n",
    "val_dataset = val_dataset.remove_columns(['review'])\n",
    "test_dataset = test_dataset.remove_columns(['review'])\n",
    "\n",
    "# 3. Загрузка модели\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-multilingual-cased',\n",
    "    num_labels=3,  # Теперь у нас 3 класса\n",
    "    problem_type=\"single_label_classification\",\n",
    "    id2label={0: \"bad\", 1: \"neutral\", 2: \"good\"},\n",
    "    label2id={\"bad\": 0, \"neutral\": 1, \"good\": 2}\n",
    ")\n",
    "\n",
    "# 4. Настройка обучения\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2.0e-05,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    "    num_train_epochs=8,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    report_to='none',\n",
    "    remove_unused_columns=True\n",
    ")\n",
    "\n",
    "# Метрики\n",
    "accuracy_metric = evaluate.load('accuracy')\n",
    "f1_metric = evaluate.load('f1')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)['accuracy']\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')['f1']\n",
    "    return {'accuracy': accuracy, 'f1': f1}\n",
    "\n",
    "# 5. Создание Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "# Обучение\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nОценка на тестовых данных...\")\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(\"\\nTest Results:\")\n",
    "print(f\"Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {test_results['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42274d2",
   "metadata": {},
   "source": [
    "Модель тренируется, но не достигает удовлетворительной производительности даже на обучающих данных. \n",
    "Текущие результаты (task accuracy ~52% validation / ~47% test) недостаточны для практического использования, \n",
    "так как близка к базовому уровню (случайное угадывание). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
